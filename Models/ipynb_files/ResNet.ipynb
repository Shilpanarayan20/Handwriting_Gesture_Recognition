{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Model\n"
      ],
      "metadata": {
        "id": "G-EKhwslUxpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, ReLU, GlobalAveragePooling2D, Add\n",
        "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, BatchNormalization, Conv2D, MaxPool2D\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "class ResnetBlock(tf.keras.Model):\n",
        "    \n",
        "\n",
        "    def __init__(self, channels: int, down_sample=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.__channels = channels\n",
        "        self.__down_sample = down_sample\n",
        "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
        "\n",
        "        KERNEL_SIZE = (3, 3)\n",
        "        INIT_SCHEME = \"he_normal\"\n",
        "\n",
        "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
        "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
        "        self.bn_1 = BatchNormalization()\n",
        "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
        "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
        "        self.bn_2 = BatchNormalization()\n",
        "        self.merge = Add()\n",
        "\n",
        "        if self.__down_sample:\n",
        "            self.res_conv = Conv2D(\n",
        "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
        "            self.res_bn = BatchNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        res = inputs\n",
        "\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.bn_1(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.bn_2(x)\n",
        "\n",
        "        if self.__down_sample:\n",
        "            res = self.res_conv(res)\n",
        "            res = self.res_bn(res)\n",
        "\n",
        "      \n",
        "        x = self.merge([x, res])\n",
        "        out = tf.nn.relu(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet18(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes, **kwargs):\n",
        "        \"\"\"\n",
        "            num_classes: number of classes in specific classification task.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n",
        "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
        "        self.init_bn = BatchNormalization()\n",
        "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
        "        self.res_1_1 = ResnetBlock(64)\n",
        "        self.res_1_2 = ResnetBlock(64)\n",
        "        self.res_2_1 = ResnetBlock(128, down_sample=True)\n",
        "        self.res_2_2 = ResnetBlock(128)\n",
        "        self.res_3_1 = ResnetBlock(256, down_sample=True)\n",
        "        self.res_3_2 = ResnetBlock(256)\n",
        "        self.res_4_1 = ResnetBlock(512, down_sample=True)\n",
        "        self.res_4_2 = ResnetBlock(512)\n",
        "        self.avg_pool = GlobalAveragePooling2D()\n",
        "        self.flat = Flatten()\n",
        "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        out = self.conv_1(inputs)\n",
        "        out = self.init_bn(out)\n",
        "        out = tf.nn.relu(out)\n",
        "        out = self.pool_2(out)\n",
        "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
        "            out = res_block(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = self.flat(out)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "nXsCWMVToDmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18(2)\n",
        "model.build(input_shape = (None,2,98,1))\n",
        "model.compile(optimizer = \"adam\",loss='binary_crossentropy', metrics=[\"accuracy\"]) \n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BfXQYtdkuHvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading"
      ],
      "metadata": {
        "id": "iTxk9j_DU4Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.load('/content/(N_W)X_Data.npy',allow_pickle=True)\n",
        "Y = np.load('/content/(N_W)Y_Data.npy',allow_pickle=True)\n",
        "X_train, X_test, y_train , y_test  = train_test_split(X, Y, test_size = 0.30, random_state = 150, shuffle=True)\n"
      ],
      "metadata": {
        "id": "xW6t5sER3yYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.asarray(X_train).astype('float32')\n",
        "X_train = tf.reshape(tf.constant(X_train), [X_train.shape[0],2,98, 1])\n",
        "y_train = tf.constant(y_train)\n",
        "y_train = y_train \n",
        "y_train = tf.one_hot(y_train, depth = 2)\n",
        "y_train = np.reshape(y_train,(8463,2))\n",
        "y_train.shape\n",
        "\n",
        "y_test = tf.constant(y_test)\n",
        "y_test = y_test \n",
        "y_test = tf.one_hot(y_test, depth = 2)\n",
        "y_test = np.reshape(y_test,(3627,2))\n",
        "y_train.shape"
      ],
      "metadata": {
        "id": "3-TmmGJ6tEqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "D-vPFBAuD8G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "5mAI24ITooyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "AMG8VVUTU7y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.fit(X_train,y_train, epochs = 75,batch_size = 64)"
      ],
      "metadata": {
        "id": "N_uB-u9S3889"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "JRuAhyh7VAQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test = np.load('/content/X_test.npy',allow_pickle=True)\n",
        "#Y_test = np.load('/content/Y_test.npy',allow_pickle=True)\n",
        "\n",
        "\n",
        "X_test = np.asarray(X_test).astype('float32')\n",
        "X_test = tf.reshape(tf.constant(X_test), [X_test.shape[0],2,98, 1])\n"
      ],
      "metadata": {
        "id": "jM1_hLHh8Dfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "YYJgdZ8i4HHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Accuracy Score = \", accuracy_score(YY_test, YY_predict))"
      ],
      "metadata": {
        "id": "a3xv3AFgaUQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/Binary_ResNet')"
      ],
      "metadata": {
        "id": "vDCX2iAGxDsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0G5Y07zYfnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = tf.keras.models.load_model('/content/sample_data')"
      ],
      "metadata": {
        "id": "HCZ4kQx7AASY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "v-isR_VpVT7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Predict = model.predict(X_test)"
      ],
      "metadata": {
        "id": "nNp9k0vyl-An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YY = np.array(y_test)\n",
        "YY_test = np.argmax(YY,axis = 1)\n",
        "\n",
        "YY_predict = np.argmax(Predict,axis = 1)\n",
        "YY_test = np.argmax(YY,axis = 1)\n",
        "\n",
        "YY_predict = np.argmax(Predict,axis = 1)"
      ],
      "metadata": {
        "id": "l860ctkel-Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YY_predict"
      ],
      "metadata": {
        "id": "lYBdSyILhlEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "cm = confusion_matrix(YY_test, YY_predict)\n",
        "\n",
        "cm"
      ],
      "metadata": {
        "id": "H4ZGJs7Ml-NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def print_confusion_matrix(confusion_matrix, class_names, figsize = (8,8),fontsize=14, normalize=True):\n",
        "     \n",
        "    if normalize:\n",
        "        confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
        "        fmt = '.2f'\n",
        "    else:\n",
        "        fmt = 'd'\n",
        "\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names,\n",
        "    )\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt= fmt)\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "IJGfjU-CmiCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A',\n",
        " #                  'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "  #                 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
        "   #                'X', 'Y', 'Z']\n",
        "\n",
        "\n",
        "#class_names = ['0,O,o','1,I,i,l ','2,Z,z','3','4','5,S,s','6,G','7','8','9,a,g,q','A',\n",
        " #                  'B', 'C,c', 'D,P,p,b', 'E,e', 'F,f', 'H,h', 'J,j ', 'K,k', 'L',\n",
        "  #                 'M,m', 'N,n', 'Q','R','T,t', 'U,V,u,v', 'W,w','X,x', 'Y,y', 'd'] \n",
        "\n",
        "#class_names = ['0','1','2','3','4','5','6','7','8','9','A',\n",
        "                # 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "                 # 'M', 'N', 'O', 'P', 'Q', 'R', 'S','T', 'U', 'V', 'W',\n",
        "                 #'X', 'Y', 'Z','a',\n",
        "                 #'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                 # 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',\n",
        "                 # 'x', 'y', 'z'] \n",
        "\n",
        "\n",
        "class_names = ['Not Writing','Writing']                   \n",
        "print_confusion_matrix(cm, class_names)\n",
        "plt.savefig('Confusion_Matix_.png', dpi=300)"
      ],
      "metadata": {
        "id": "oH8GFyVHZ9_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#report = classification_report(Y_test, preds,target_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A',\n",
        " #                  'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "  #                 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
        "   #                'X', 'Y', 'Z'])\n",
        "\n",
        "#report = classification_report(YY_test, YY_predict,target_names=['0,O,o','1,I,i,l ','2,Z,z','3','4','5,S,s','6,G','7','8','9,a,g,q','A',\n",
        " #                  'B', 'C,c', 'D,P,p,b', 'E,e', 'F,f', 'H,h', 'J,j ', 'K,k', 'L',\n",
        "  #                 'M,m', 'N,n', 'Q','R','T,t', 'U,V,u,v', 'W,w','X,x', 'Y,y', 'd']) \n",
        "\n",
        "#report = classification_report(YY_test, YY_predict,target_names= ['0','1','2','3','4','5','6','7','8','9','A',\n",
        " #                'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "  #               'M', 'N', 'O', 'P', 'Q', 'R', 'S','T', 'U', 'V', 'W',\n",
        "   #               'X', 'Y', 'Z','a',\n",
        "    #             'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "     #             'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',\n",
        "      #            'x', 'y', 'z']) \n",
        "\n",
        "\n",
        "report = classification_report(YY_test, YY_predict,target_names= ['Not Writing','Writing'])\n",
        "print(report)"
      ],
      "metadata": {
        "id": "9hnGV4Ypmt5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr\n",
        "print(report)"
      ],
      "metadata": {
        "id": "Zb7MXYT-gi58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Classification_Report.txt', 'w') as f:\n",
        "    f.write(cap.stdout)"
      ],
      "metadata": {
        "id": "twknyQnAgmc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "correct = [pred == true for pred, true in zip(YY_predict, YY_test)]\n",
        "correct = np.array(correct).flatten()\n",
        "print(Counter(correct))"
      ],
      "metadata": {
        "id": "UNMbE4-YmiGH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}